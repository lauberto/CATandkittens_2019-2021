{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import tempfile\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = mysql.connector.connect(user='andr',\n",
    "                              password='rstq!2Ro',\n",
    "                              host='127.0.0.1',\n",
    "                              database='cat_db',\n",
    "                              auth_plugin='mysql_native_password'\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = con.cursor(dictionary=True, buffered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "SELECT id_text, morph, pos FROM\n",
    "(SELECT id_text, id_unigram FROM words) AS a JOIN\n",
    "(SELECT morph, id_unigram, lemma FROM unigrams) AS b ON a.id_unigram = b.id_unigram JOIN\n",
    "(SELECT id_lemmas, id_pos FROM lemmas) AS c ON lemma = id_lemmas JOIN\n",
    "(SELECT pos, id_pos FROM pos) AS d ON c.id_pos = d.id_pos\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('regtab.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id_text', 'morph', 'pos'])\n",
    "    for dictionary in rows:\n",
    "        writer.writerow([dictionary['id_text'], dictionary['morph'], dictionary['pos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('regtab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor(dictionary):\n",
    "    factor = 1.0/sum(dictionary.values())\n",
    "    for k in dictionary:\n",
    "        dictionary[k] = dictionary[k]*factor\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morphodict:\n",
    "    def __init__(self):\n",
    "        self.impVERB = 0\n",
    "        self.perVERB = 0\n",
    "        self.prsVERB = 0\n",
    "        self.pstVERB = 0\n",
    "        self.futVERB = 0\n",
    "        self.finVERB = 0\n",
    "        self.partVERB = 0\n",
    "        self.passVOICE = 0\n",
    "        self.midVOICE = 0\n",
    "        self.actVOICE = 0\n",
    "        self.inADJ = 0\n",
    "        self.anADJ = 0\n",
    "        self.inNOUN = 0\n",
    "        self.anNOUN = 0\n",
    "        self.AUX = 0\n",
    "        \n",
    "    def toDict(self):\n",
    "        res={\"impVERB\":self.impVERB, \"perVERB\":self.perVERB, \"prsVERB\":self.prsVERB, \"pstVERB\":self.pstVERB,\\\n",
    "             \"futVERB\":self.futVERB, \"finVERB\":self.finVERB, \"partVERB\":self.partVERB, \"passVOICE\":self.passVOICE, \\\n",
    "             \"midVOICE\":self.midVOICE, \"actVOICE\":self.actVOICE, \"inADJ\":self.inADJ, \"anADJ\":self.anADJ, \\\n",
    "             \"inNOUN\":self.inNOUN, \"anNOUN\":self.anNOUN, \"AUX\":self.AUX}\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagcounter(id_text, df):\n",
    "    morcounter = Morphodict()\n",
    "    current = df.loc[df['id_text'] == id_text]\n",
    "    for i, row in current.iterrows():\n",
    "        morph_split = row['morph'].split('|')\n",
    "#         print(morph_split)\n",
    "        for chunk in morph_split:\n",
    "            if row['pos'] == 'VERB':\n",
    "                if 'Aspect=Imp' in chunk:\n",
    "                    morcounter.impVERB = morcounter.impVERB + 1\n",
    "                elif 'Aspect=Perf' in chunk:\n",
    "                    morcounter.perVERB = morcounter.perVERB + 1\n",
    "                elif 'Tense=Pres' in chunk:\n",
    "                    morcounter.prsVERB = morcounter.prsVERB + 1\n",
    "                elif 'Tense=Past' in chunk:\n",
    "                    morcounter.pstVERB = morcounter.pstVERB + 1\n",
    "                elif 'Tense=Futr' in chunk:\n",
    "                    morcounter.futVERB = morcounter.futVERB + 1\n",
    "                elif 'VerbForm=Fin' in chunk:\n",
    "                    morcounter.finVERB = morcounter.finVERB + 1\n",
    "                elif 'VerbForm=Part' in chunk:\n",
    "                    morcounter.partVERB = morcounter.partVERB + 1\n",
    "                elif 'Voice=Pass' in chunk:\n",
    "                    morcounter.passVOICE = morcounter.passVOICE + 1\n",
    "                elif 'Voice=Mid' in chunk:\n",
    "                    morcounter.midVOICE = morcounter.midVOICE + 1\n",
    "                elif 'Voice=Act' in chunk:\n",
    "                    morcounter.actVOICE = morcounter.actVOICE + 1\n",
    "                else: \n",
    "                    continue\n",
    "                        \n",
    "            elif row['pos'] == 'ADJ':\n",
    "                if 'Animacy=Inan' in chunk:\n",
    "                    morcounter.inADJ = morcounter.inADJ + 1\n",
    "                elif 'Animacy=Anim' in chunk:\n",
    "                    morcounter.anADJ = morcounter.anADJ + 1\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            elif row['pos'] == 'NOUN':\n",
    "                if 'Animacy=Inan' in chunk:\n",
    "                    morcounter.inNOUN = morcounter.inNOUN + 1\n",
    "                elif 'Animacy=Anim' in chunk:\n",
    "                    morcounter.anNOUN = morcounter.anNOUN + 1\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            elif row['pos'] == 'AUX':\n",
    "                morcounter.AUX = morcounter.AUX + 1\n",
    "                \n",
    "            else: \n",
    "                continue\n",
    "    morcounter = morcounter.toDict()\n",
    "    return factor(morcounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'impVERB': 0.07820383451059536,\n",
       " 'perVERB': 0.042885973763874874,\n",
       " 'prsVERB': 0.06685166498486378,\n",
       " 'pstVERB': 0.03229061553985873,\n",
       " 'futVERB': 0.0,\n",
       " 'finVERB': 0.05676084762865792,\n",
       " 'partVERB': 0.04112008072653885,\n",
       " 'passVOICE': 0.034308779011099896,\n",
       " 'midVOICE': 0.014631685166498487,\n",
       " 'actVOICE': 0.07214934409687185,\n",
       " 'inADJ': 0.018415741675075682,\n",
       " 'anADJ': 0.0,\n",
       " 'inNOUN': 0.5035317860746721,\n",
       " 'anNOUN': 0.01160443995963673,\n",
       " 'AUX': 0.027245206861755803}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagcounter(691, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphlibrary(text_ids, df):\n",
    "    textholder = []\n",
    "    for text_id in text_ids:\n",
    "        textholder.append(tagcounter(text_id, df))\n",
    "    return textholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student texts for negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse, parse_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(filename):\n",
    "    \"\"\"\n",
    "    Yields a sentence from conllu tree with its tags\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    >>> for i in parser('/content/gdrive/My Drive/Новые conll по доменам/NewVers/CleanedPsyEdu.conllu'):\n",
    "      print(i)   \n",
    "    TokenList<Музыка, звучит, отовсюду, независимо, от, нашего, желания, или, нежелания, слушать, ее, .>\n",
    "    \"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    tree = parse(data)\n",
    "    for token in tree:\n",
    "        yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(tree):\n",
    "    \"\"\"\n",
    "    tree - generator of sentences (TokenLists) from conllu tree\n",
    "\n",
    "    words, list is a list of all tokens we need from the tree\n",
    "    size, int is a number of all words in the domain\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for sentence in tree:\n",
    "        for token in sentence:\n",
    "#             print(token)\n",
    "            if token['form'] != '_' and token['upostag'] != '_' and token['upostag']!='NONLEX' and token['form'] not in r'[]\\/':\n",
    "                for wordform in token['form'].lower().split():\n",
    "                    words.append((wordform, token['lemma'], token['feats'], token['upostag']))\n",
    "    size = len(words)\n",
    "    return words, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'C:\\Users\\Andrea\\Desktop\\stud_textVSscie_text\\RULEC_PARSED\\Max_FINISHED_PARSED\\Max_HL_AM_2011-2012_Week_10_1_paragraph+_expository_non-timed.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree = parser(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words, size = get_words(tree)\n",
    "# del tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagcounter4studtexts2(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detagger(words):\n",
    "    tag_dict = {}\n",
    "    for word in words:\n",
    "        if word[2]:\n",
    "            for tag in list(word[2].items()):\n",
    "                full_tag = '{}={}'.format(tag[0], tag[1])\n",
    "                if word[3] == 'VERB':\n",
    "                    if 'VERB' in tag_dict:\n",
    "                        tag_dict['VERB'].append(full_tag)\n",
    "                    else:\n",
    "                        tag_dict['VERB'] = [full_tag]\n",
    "                elif word[3] == 'ADJ':\n",
    "                    if 'ADJ' in tag_dict:\n",
    "                        tag_dict['ADJ'].append(full_tag)\n",
    "                    else:\n",
    "                        tag_dict['ADJ'] = [full_tag]\n",
    "                elif word[3] == 'NOUN':\n",
    "                    if 'NOUN' in tag_dict:\n",
    "                        tag_dict['NOUN'].append(full_tag)\n",
    "                    else:\n",
    "                        tag_dict['NOUN'] = [full_tag]\n",
    "                elif word[3] == 'AUX':\n",
    "                    tag_dict['AUX'] = tag_dict['AUX'] + 1 if 'AUX' in tag_dict else 1\n",
    "                else:\n",
    "                    continue\n",
    "    return tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagcounter4studtexts(words):\n",
    "    morcounter = Morphodict()\n",
    "    tagdict = detagger(words)\n",
    "    if 'VERB' in tagdict:\n",
    "        for value in tagdict['VERB']:\n",
    "            if 'Aspect=Imp' in value:\n",
    "                morcounter.impVERB = morcounter.impVERB + 1\n",
    "            elif 'Aspect=Perf' in value:\n",
    "                morcounter.perVERB = morcounter.perVERB + 1\n",
    "            elif 'Tense=Pres' in value:\n",
    "                morcounter.prsVERB = morcounter.prsVERB + 1\n",
    "            elif 'Tense=Past' in value:\n",
    "                morcounter.pstVERB = morcounter.pstVERB + 1\n",
    "            elif 'Tense=Futr' in value:\n",
    "                morcounter.futVERB = morcounter.futVERB + 1\n",
    "            elif 'VerbForm=Fin' in value:\n",
    "                morcounter.finVERB = morcounter.finVERB + 1\n",
    "            elif 'VerbForm=Part' in value:\n",
    "                morcounter.partVERB = morcounter.partVERB + 1\n",
    "            elif 'Voice=Pass' in value:\n",
    "                morcounter.passVOICE = morcounter.passVOICE + 1\n",
    "            elif 'Voice=Mid' in value:\n",
    "                morcounter.midVOICE = morcounter.midVOICE + 1\n",
    "            elif 'Voice=Act' in value:\n",
    "                morcounter.actVOICE = morcounter.actVOICE + 1\n",
    "            else: \n",
    "                continue\n",
    "                        \n",
    "    if 'ADJ' in tagdict:\n",
    "        for value in tagdict['ADJ']:\n",
    "            if 'Animacy=Inan' in value:\n",
    "                morcounter.inADJ = morcounter.inADJ + 1\n",
    "            elif 'Animacy=Anim' in value:\n",
    "                morcounter.anADJ = morcounter.anADJ + 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    if 'NOUN' in tagdict:\n",
    "        for value in tagdict['NOUN']:\n",
    "            if 'Animacy=Inan' in value:\n",
    "                morcounter.inNOUN = morcounter.inNOUN + 1\n",
    "            elif 'Animacy=Anim' in value:\n",
    "                morcounter.anNOUN = morcounter.anNOUN + 1\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    if 'AUX' in tagdict:\n",
    "        morcounter.AUX = tagdict['AUX']\n",
    "    else:\n",
    "        morcounter.AUX = 0\n",
    "    morcounter = morcounter.toDict()\n",
    "    if sum(morcounter.values()) > 150:\n",
    "        return factor(morcounter)\n",
    "    else:\n",
    "        print('The text was too short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_STUDTEXTS_RULEC = r'C:\\Users\\Andrea\\Desktop\\stud_textVSscie_text\\RULEC_PARSED'\n",
    "PATH_TO_STUDTEXT = r'C:\\Users\\Andrea\\Desktop\\stud_textVSscie_text\\Student_texts_for_experiments\\stud_txt\\conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = [PATH_TO_STUDTEXTS_RULEC, PATH_TO_STUDTEXT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphlibrary4students(paths_to_student_text_dir):\n",
    "    textholder = []\n",
    "    for path in paths_to_student_text_dir:\n",
    "        for dir_ in os.listdir(path):\n",
    "            for filepath in os.listdir(os.path.join(path, dir_)):\n",
    "                tree = parser(os.path.join(path, dir_, filepath))\n",
    "                words, size = get_words(tree)\n",
    "                del tree\n",
    "                textholder.append(tagcounter4studtexts(words))\n",
    "    return textholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(df.id_text.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildthetab(df, paths_to_student_text_dir):\n",
    "    \n",
    "    # list of dictionaries with morphological parameters for academic texts\n",
    "    text_ids = list(df.id_text.unique())\n",
    "    academic_txt_datalibrary = morphlibrary(text_ids, df)\n",
    "    \n",
    "    # list of dictionaries for student texts\n",
    "    student_txt_datalibrary = morphlibrary4students(paths_to_student_text_dir)\n",
    "    \n",
    "    with open('datab.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['impVERB', 'perVERB', 'prsVERB', 'pstVERB', 'futVERB', 'finVERB', 'partVERB',\n",
    "                        'passVOICE', 'midVOICE', 'actVOICE', 'inADJ', 'anADJ', 'inNOUN', 'anNOUN',\n",
    "                        'AUX', 'academicity'])\n",
    "        for _dict in academic_txt_datalibrary:\n",
    "            writer.writerow([_dict['impVERB'], _dict['perVERB'], _dict['prsVERB'], _dict['pstVERB'],\n",
    "                            _dict['futVERB'], _dict['finVERB'], _dict['partVERB'], _dict['passVOICE'],\n",
    "                            _dict['midVOICE'], _dict['actVOICE'], _dict['inADJ'], _dict['anADJ'],\n",
    "                            _dict['inNOUN'], _dict['anNOUN'], _dict['AUX'], 1])\n",
    "        for _dict in student_txt_datalibrary:\n",
    "            try:\n",
    "                writer.writerow([_dict['impVERB'], _dict['perVERB'], _dict['prsVERB'], _dict['pstVERB'],\n",
    "                            _dict['futVERB'], _dict['finVERB'], _dict['partVERB'], _dict['passVOICE'],\n",
    "                            _dict['midVOICE'], _dict['actVOICE'], _dict['inADJ'], _dict['anADJ'],\n",
    "                            _dict['inNOUN'], _dict['anNOUN'], _dict['AUX'], 0])\n",
    "            except TypeError:\n",
    "                print(_dict)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "The text was too short\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "buildthetab(df, PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho_df = pd.read_csv('datab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(920, 16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morpho_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impVERB</th>\n",
       "      <th>perVERB</th>\n",
       "      <th>prsVERB</th>\n",
       "      <th>pstVERB</th>\n",
       "      <th>futVERB</th>\n",
       "      <th>finVERB</th>\n",
       "      <th>partVERB</th>\n",
       "      <th>passVOICE</th>\n",
       "      <th>midVOICE</th>\n",
       "      <th>actVOICE</th>\n",
       "      <th>inADJ</th>\n",
       "      <th>anADJ</th>\n",
       "      <th>inNOUN</th>\n",
       "      <th>anNOUN</th>\n",
       "      <th>AUX</th>\n",
       "      <th>academicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.078204</td>\n",
       "      <td>0.042886</td>\n",
       "      <td>0.066852</td>\n",
       "      <td>0.032291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056761</td>\n",
       "      <td>0.041120</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.014632</td>\n",
       "      <td>0.072149</td>\n",
       "      <td>0.018416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503532</td>\n",
       "      <td>0.011604</td>\n",
       "      <td>0.027245</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.101195</td>\n",
       "      <td>0.032431</td>\n",
       "      <td>0.092416</td>\n",
       "      <td>0.027067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081687</td>\n",
       "      <td>0.037064</td>\n",
       "      <td>0.029993</td>\n",
       "      <td>0.032431</td>\n",
       "      <td>0.071202</td>\n",
       "      <td>0.009998</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.452573</td>\n",
       "      <td>0.019507</td>\n",
       "      <td>0.012192</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.077165</td>\n",
       "      <td>0.030377</td>\n",
       "      <td>0.055168</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060405</td>\n",
       "      <td>0.024092</td>\n",
       "      <td>0.023743</td>\n",
       "      <td>0.015712</td>\n",
       "      <td>0.068087</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.594972</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>0.005936</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.091710</td>\n",
       "      <td>0.047824</td>\n",
       "      <td>0.071643</td>\n",
       "      <td>0.039760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072018</td>\n",
       "      <td>0.040885</td>\n",
       "      <td>0.032633</td>\n",
       "      <td>0.020818</td>\n",
       "      <td>0.086084</td>\n",
       "      <td>0.011065</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.428357</td>\n",
       "      <td>0.015004</td>\n",
       "      <td>0.042011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.094278</td>\n",
       "      <td>0.037057</td>\n",
       "      <td>0.076294</td>\n",
       "      <td>0.024523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064305</td>\n",
       "      <td>0.036512</td>\n",
       "      <td>0.032153</td>\n",
       "      <td>0.017439</td>\n",
       "      <td>0.081744</td>\n",
       "      <td>0.014714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460490</td>\n",
       "      <td>0.014714</td>\n",
       "      <td>0.045777</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    impVERB   perVERB   prsVERB   pstVERB  futVERB   finVERB  partVERB  passVOICE  midVOICE  actVOICE     inADJ     anADJ    inNOUN    anNOUN       AUX  academicity\n",
       "0  0.078204  0.042886  0.066852  0.032291      0.0  0.056761  0.041120   0.034309  0.014632  0.072149  0.018416  0.000000  0.503532  0.011604  0.027245            1\n",
       "1  0.101195  0.032431  0.092416  0.027067      0.0  0.081687  0.037064   0.029993  0.032431  0.071202  0.009998  0.000244  0.452573  0.019507  0.012192            1\n",
       "2  0.077165  0.030377  0.055168  0.022346      0.0  0.060405  0.024092   0.023743  0.015712  0.068087  0.008380  0.000000  0.594972  0.013617  0.005936            1\n",
       "3  0.091710  0.047824  0.071643  0.039760      0.0  0.072018  0.040885   0.032633  0.020818  0.086084  0.011065  0.000188  0.428357  0.015004  0.042011            1\n",
       "4  0.094278  0.037057  0.076294  0.024523      0.0  0.064305  0.036512   0.032153  0.017439  0.081744  0.014714  0.000000  0.460490  0.014714  0.045777            1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morpho_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impVERB</th>\n",
       "      <th>perVERB</th>\n",
       "      <th>prsVERB</th>\n",
       "      <th>pstVERB</th>\n",
       "      <th>futVERB</th>\n",
       "      <th>finVERB</th>\n",
       "      <th>partVERB</th>\n",
       "      <th>passVOICE</th>\n",
       "      <th>midVOICE</th>\n",
       "      <th>actVOICE</th>\n",
       "      <th>inADJ</th>\n",
       "      <th>anADJ</th>\n",
       "      <th>inNOUN</th>\n",
       "      <th>anNOUN</th>\n",
       "      <th>AUX</th>\n",
       "      <th>academicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095960</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.035354</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.095960</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.084848</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096970</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>0.078788</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>0.090323</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.077419</td>\n",
       "      <td>0.051613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090323</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.090323</td>\n",
       "      <td>0.025806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335484</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.039286</td>\n",
       "      <td>0.067857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096429</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.067857</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364286</td>\n",
       "      <td>0.092857</td>\n",
       "      <td>0.096429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>0.110526</td>\n",
       "      <td>0.057895</td>\n",
       "      <td>0.047368</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.121053</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.068421</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>915</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.106481</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.032407</td>\n",
       "      <td>0.157407</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>916</td>\n",
       "      <td>0.101399</td>\n",
       "      <td>0.059441</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.024476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.146853</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244755</td>\n",
       "      <td>0.101399</td>\n",
       "      <td>0.073427</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>917</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.094444</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>918</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.082621</td>\n",
       "      <td>0.042735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136752</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.048433</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.145299</td>\n",
       "      <td>0.122507</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>919</td>\n",
       "      <td>0.131653</td>\n",
       "      <td>0.044818</td>\n",
       "      <td>0.030812</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123249</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.022409</td>\n",
       "      <td>0.151261</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179272</td>\n",
       "      <td>0.075630</td>\n",
       "      <td>0.134454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      impVERB   perVERB   prsVERB   pstVERB  futVERB   finVERB  partVERB  passVOICE  midVOICE  actVOICE     inADJ     anADJ    inNOUN    anNOUN       AUX  academicity\n",
       "600  0.106061  0.025253  0.101010  0.020202      0.0  0.095960  0.030303   0.035354  0.040404  0.055556  0.000000  0.000000  0.343434  0.095960  0.050505            0\n",
       "601  0.109091  0.042424  0.084848  0.030303      0.0  0.096970  0.018182   0.018182  0.000000  0.133333  0.018182  0.000000  0.327273  0.078788  0.042424            0\n",
       "602  0.090323  0.064516  0.077419  0.051613      0.0  0.090323  0.045161   0.045161  0.019355  0.090323  0.025806  0.000000  0.335484  0.019355  0.045161            0\n",
       "603  0.085714  0.028571  0.039286  0.067857      0.0  0.096429  0.007143   0.010714  0.035714  0.067857  0.007143  0.000000  0.364286  0.092857  0.096429            0\n",
       "604  0.110526  0.057895  0.047368  0.078947      0.0  0.126316  0.010526   0.005263  0.042105  0.121053  0.015789  0.000000  0.252632  0.068421  0.063158            0\n",
       "..        ...       ...       ...       ...      ...       ...       ...        ...       ...       ...       ...       ...       ...       ...       ...          ...\n",
       "915  0.152778  0.041667  0.106481  0.027778      0.0  0.138889  0.004630   0.004630  0.032407  0.157407  0.004630  0.000000  0.138889  0.092593  0.097222            0\n",
       "916  0.101399  0.059441  0.090909  0.024476      0.0  0.136364  0.000000   0.000000  0.013986  0.146853  0.006993  0.000000  0.244755  0.101399  0.073427            0\n",
       "917  0.111111  0.033333  0.094444  0.016667      0.0  0.105556  0.005556   0.005556  0.016667  0.122222  0.000000  0.000000  0.177778  0.155556  0.155556            0\n",
       "918  0.119658  0.051282  0.082621  0.042735      0.0  0.136752  0.002849   0.002849  0.048433  0.119658  0.002849  0.002849  0.145299  0.122507  0.119658            0\n",
       "919  0.131653  0.044818  0.030812  0.095238      0.0  0.123249  0.002801   0.002801  0.022409  0.151261  0.005602  0.000000  0.179272  0.075630  0.134454            0\n",
       "\n",
       "[320 rows x 16 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morpho_df.loc[morpho_df['academicity'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
